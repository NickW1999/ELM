{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec90b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general packages\n",
    "import os\n",
    "import timeit\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "#possible package for feature extraction\n",
    "import entropy as ent\n",
    "import librosa.feature as lrf \n",
    "\n",
    "#packages for models\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#packages for feature selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d741543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for feature extraction\n",
    "# 25th Percentile\n",
    "def q25(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "# 75th Percentile\n",
    "def q75(x):\n",
    "    return x.quantile(0.75)\n",
    "#\n",
    "def Featuremfcc(X):\n",
    "    X=np.array(X)\n",
    "    flag = not np.any(X)\n",
    "    if flag:\n",
    "        return 0\n",
    "    if np.isfinite(X).all()!=True:\n",
    "        return 0\n",
    "    return np.sum(lrf.mfcc(y=X,sr=40, n_mfcc=1))\n",
    "#spectral entropy\n",
    "def Featureent(X):\n",
    "\n",
    "    X=np.array(X)\n",
    "    if np.isfinite(X).all()!=True:\n",
    "        return 0\n",
    "    return ent.spectral_entropy(np.array(X),sf=40,method='fft', normalize=False)\n",
    "#zero crossing rate\n",
    "def Featurezcr(X):\n",
    "\n",
    "    X=np.array(X)\n",
    "    if np.isfinite(X).all()!=True:\n",
    "        return 0\n",
    "    flag = not np.any(X)\n",
    "    if flag:\n",
    "        return 0\n",
    "    return np.mean(lrf.zero_crossing_rate(X, frame_length=40, hop_length= 100))\n",
    "\n",
    "                                \n",
    "#spectral roll off                                \n",
    "def Featurespecrolloff(X):\n",
    "\n",
    "    X=np.array(X)\n",
    "    if np.isfinite(X).all()!=True:\n",
    "        return 0\n",
    "    flag = not np.any(X)\n",
    "    if flag:\n",
    "        return 0\n",
    "    return np.mean(lrf.spectral_rolloff(y=X, sr=40, hop_length=100, roll_percent=0.9))\n",
    "                                  \n",
    "                                  \n",
    "#spectral bandwidth                                  \n",
    "def Featurespecbandwidth(X):\n",
    "    X=np.array(X)\n",
    "\n",
    "    flag = not np.any(X)\n",
    "    if flag:\n",
    "        return 0\n",
    "    return np.mean(lrf.spectral_bandwidth(y=X, sr=40, hop_length=100) )                                 \n",
    "#spectral flatness                                  \n",
    "def Featurespecflatness(X):\n",
    "\n",
    "    X=np.array(X)\n",
    "    if np.isfinite(X).all()!=True:\n",
    "        return 0\n",
    "    flag = not np.any(X)\n",
    "    if flag:\n",
    "        return 0\n",
    "    return np.mean(lrf.spectral_flatness(y=X, hop_length=100)   )                                 \n",
    "                                \n",
    "                                \n",
    "#spectral centroid                                \n",
    "def FeatureSpectralCentroid(X, f_s=40): #works\n",
    "\n",
    "    isSpectrum = X.ndim == 1\n",
    "\n",
    "    # X = X**2 removed for consistency with book\n",
    "    X=np.array(X)\n",
    "    norm = X.sum(axis=0, keepdims=True)\n",
    "    norm[norm == 0] = 1\n",
    "\n",
    "    vsc = np.dot(np.arange(0, X.shape[0]), X) / norm\n",
    "\n",
    "    # convert from index to Hz\n",
    "    vsc = vsc / (X.shape[0] - 1) * f_s / 2\n",
    "\n",
    "    # if input is a spectrum, output scaler else if spectrogram, output 1d array\n",
    "    vsc = np.squeeze(vsc) if isSpectrum else np.squeeze(vsc, axis=0)\n",
    "\n",
    "    return vsc\n",
    "#spectral spread\n",
    "def FeatureSpectralSpread(X, f_s=40):#works\n",
    "\n",
    "    isSpectrum = X.ndim == 1\n",
    "    if isSpectrum:\n",
    "        X = np.expand_dims(X, axis=1)\n",
    "\n",
    "    # get spectral centroid as index\n",
    "    vsc = FeatureSpectralCentroid(X, f_s) * 2 / f_s * (X.shape[0] - 1)\n",
    "\n",
    "    # X = X**2 removed for consistency with book\n",
    "\n",
    "    norm = X.sum(axis=0)\n",
    "    norm[norm == 0] = 1\n",
    "\n",
    "    # compute spread\n",
    "    vss = np.zeros(X.shape[1])\n",
    "    indices = np.arange(0, X.shape[0])\n",
    "    for n in range(0, X.shape[1]):\n",
    "        vss[n] = np.dot((indices - vsc[n])**2, X[:, n]) / norm[n]\n",
    "\n",
    "    vss = np.sqrt(vss)\n",
    "\n",
    "    # convert from index to Hz\n",
    "    vss = vss / (X.shape[0] - 1) * f_s / 2\n",
    "\n",
    "    return np.squeeze(vss) if isSpectrum else vss\n",
    "#spectral flux\n",
    "def FeatureSpectralFlux(X, f_s=40):#works\n",
    "\n",
    "    isSpectrum = X.ndim == 1\n",
    "    if isSpectrum:\n",
    "        X = np.expand_dims(X, axis=1)\n",
    "\n",
    "    # difference spectrum (set first diff to zero)\n",
    "    X = np.c_[X[:, 0], X]\n",
    "\n",
    "    afDeltaX = np.diff(X, 1, axis=1)\n",
    "\n",
    "    # flux\n",
    "    vsf = np.sqrt((afDeltaX**2).sum(axis=0)) / X.shape[0]\n",
    "\n",
    "    return np.squeeze(vsf) if isSpectrum else vsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a folder for all data and results\n",
    "new_folder_list=['data2/label','data2/data', 'data2/feature','data2/pca','data2/results']\n",
    "for new_folder in new_folder_list:\n",
    "    if not os.path.exists(new_folder):\n",
    "        os.makedirs(new_folder)\n",
    "        print(new_folder + ' has been created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfebe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to generate basic features\n",
    "def data_separation():\n",
    "    tic= timeit.default_timer()\n",
    "    global listpreproces\n",
    "    data_df_list=[]\n",
    "    data_df=pd.read_csv('data/all_accelerometer_data_pids_13.csv', parse_dates=True, index_col='time')\n",
    "    data_df.drop(data_df[data_df.index < 1].index, inplace=True)\n",
    "    data_df.index=pd.to_datetime(data_df.index, unit='ms')\n",
    "    pids = list(set(data_df['pid']))\n",
    "    sample_rate = 40 # hertz\n",
    "    subject_height = 170 # centimeters\n",
    "    for pid in pids:\n",
    "        df = data_df[data_df['pid'] == pid]\n",
    "        x=df['x'].copy()\n",
    "        y=df['y'].copy()\n",
    "        z=df['z'].copy()\n",
    "        #first difference\n",
    "        df['delta x'] = x-x.shift(1)\n",
    "        df['delta y'] = y-y.shift(1)\n",
    "        df['delta z'] = z-z.shift(1)\n",
    "        #squared\n",
    "        df['x^2']=x**2\n",
    "        df['y^2']=y**2\n",
    "        df['z^2']=z**2\n",
    "        #abs\n",
    "        df['abs x'] = np.absolute(x)\n",
    "        df['abs y'] = np.absolute(y)\n",
    "        df['abs z'] = np.absolute(z)\n",
    "        #energy feature\n",
    "        df['energy']=np.sqrt(x**2+y**2+z**2)\n",
    "        #prod\n",
    "        df['prod']= x*y*z\n",
    "        #add chebychev\n",
    "        b, a = signal.cheby2(15, 40, [0.1,0.9], 'bandstop')\n",
    "        filteredx = signal.lfilter(b,a,x).copy()\n",
    "        filteredy = signal.lfilter(b,a,y).copy()\n",
    "        filteredz = signal.lfilter(b,a,z).copy()\n",
    "        df['fx'] = filteredx\n",
    "        df['fy'] = filteredy\n",
    "        df['fz'] = filteredz        \n",
    "        #first differences\n",
    "        df['delta fx'] = np.concatenate(([0],np.diff(filteredx)))\n",
    "        df['delta fy'] = np.concatenate(([0],np.diff(filteredy)))\n",
    "        df['delta fz'] = np.concatenate(([0],np.diff(filteredz)))\n",
    "        #squared\n",
    "        df['fx^2']=filteredx**2\n",
    "        df['fy^2']=filteredy**2\n",
    "        df['fz^2']=filteredz**2\n",
    "        #abs\n",
    "        df['abs fx'] = np.absolute(filteredx)\n",
    "        df['abs fy'] = np.absolute(filteredy)\n",
    "        df['abs fz'] = np.absolute(filteredz)\n",
    "        #energy feature\n",
    "        df['fenergy']=np.sqrt(filteredx**2+filteredy**2+filteredz**2)\n",
    "        #prod\n",
    "        df['prod']= filteredx*filteredy*filteredz\n",
    "        \n",
    "        listpreproces=df.columns\n",
    "        filename='data2/data/'+pid+'.csv'\n",
    "        df.dropna(inplace=True)\n",
    "        df.to_csv(filename)\n",
    "        \n",
    "        print(filename +\" has been created\")\n",
    "    toc= timeit.default_timer()\n",
    "    print(toc-tic)\n",
    "\n",
    "\n",
    "data_separation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28861723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranform the target into a classification\n",
    "def add_label():\n",
    "    label_df_list=[]\n",
    "    pids=[]\n",
    "    tic= timeit.default_timer()\n",
    "    csv_files = glob('data/clean_tac/*.csv')\n",
    "    for csv_file in csv_files:\n",
    "        df=pd.read_csv(csv_file, parse_dates=True, index_col='timestamp')\n",
    "        pid=csv_file[len('data/clean_tac\\\\'):-len('_clean_TAC.csv')]\n",
    "        pids.append(pid)\n",
    "        df['label']= 0\n",
    "        df.loc[df['TAC_Reading']>=.08,'label']= 1\n",
    "        df.index=pd.to_datetime(df.index, unit='s')\n",
    "        df=df.resample('1S').pad()\n",
    "        filename='data2/label/'+ pid+'.csv'\n",
    "        df.to_csv(filename)\n",
    "        print (filename +' has been created')\n",
    "    toc= timeit.default_timer()\n",
    "    print(toc-tic)\n",
    "    return pids\n",
    "\n",
    "pids=add_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7552669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to show how a model performs\n",
    "def classification_report_df(y_true, y_pred, labels,name=\"\"):\n",
    "    global tic\n",
    "    report = classification_report(y_true, y_pred, labels=labels, digits=4, zero_division=0)\n",
    "    report=report.replace(\"\\nweighted avg\", \"\\n weighted_avg\")\n",
    "    report=report.replace(\"\\n weighted avg\", \"\\n weighted_avg\")\n",
    "    report=report.replace(\"macro avg\", \"macro_avg\")    \n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    toc= timeit.default_timer()\n",
    "    for line in lines[2:]:\n",
    "        row_data = line.split(None)\n",
    "        if len(row_data)!=5:\n",
    "            continue\n",
    "        row = {}        \n",
    "        row['Class'] = row_data[0]\n",
    "        row['Precision'] = 100*float(row_data[1])\n",
    "        row['Recall'] = 100*float(row_data[2])\n",
    "        row['F1_score'] = 100*float(row_data[3])\n",
    "        row['Support'] =float(row_data[4])\n",
    "        row['time in min']=(toc-tic)/60\n",
    "        report_data.append(row)\n",
    "    df_clf_report = pd.DataFrame.from_dict(report_data)\n",
    "    df_clf_report.to_csv('data2/results/classification_report '+name+'.csv', index = False)\n",
    "    return df_clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fe011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract final features\n",
    "def extract_features(df, time_interval):\n",
    "    global listpreproces\n",
    "    tic= timeit.default_timer()\n",
    "    listsortfeatures=[np.nansum, np.nanmin, np.nanmax, np.nanmean, np.nanmedian,\n",
    "                      np.nanstd, np.nanvar,scipy.stats.skew,scipy.stats.kurtosis, q75, q25,\n",
    "                      FeatureSpectralCentroid,FeatureSpectralSpread,FeatureSpectralFlux,\n",
    "                     Featureent, Featurezcr, Featurespecrolloff,\n",
    "                      Featurespecflatness]#missing Featuremfcc,Featurespecbandwidth\n",
    "    dictagg={i:listsortfeatures for i in listpreproces}\n",
    "    toc= timeit.default_timer()\n",
    "    print(toc-tic)\n",
    "    return df.resample(time_interval).agg(dictagg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_label(pids,interval):\n",
    "    global listpreproces\n",
    "    global dim\n",
    "\n",
    "    for pid in pids:        \n",
    "        in_filename_data='data/data/'+ pid+'.csv'\n",
    "        in_filename_label='data/label/'+ pid+'.csv'\n",
    "        out_filename='data2/feature/'+ pid+'.csv'\n",
    "        data_df=pd.read_csv(in_filename_data, parse_dates=True, index_col='time', usecols= list(listpreproces).append('time'))    \n",
    "        label_df=pd.read_csv(in_filename_label, parse_dates=True, index_col='timestamp', usecols=['timestamp', 'label'])\n",
    "        listpreproces=list(data_df.columns)[1:]\n",
    "        feature_df=extract_features(data_df, interval)        \n",
    "        feature_label_df = pd.concat([feature_df, label_df['label']], axis=1, join=\"inner\")\n",
    "        feature_label_df.to_csv(out_filename, index_label='time')\n",
    "        print(out_filename + ' has been created')  \n",
    "    dim=len(data_df.columns)\n",
    "#extract_features_label(pids, '1S')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153ace74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all data in a data frame\n",
    "def load_features_label(pids):\n",
    "    global dim\n",
    "    tic= timeit.default_timer()\n",
    "    dim= len(pd.read_csv('data2/feature/'+ pids[0].iloc[0]+'.csv').columns)-1\n",
    "    df=pd.DataFrame(np.repeat(None, dim)).T\n",
    "    for pid in pids[0]:\n",
    "        filename='data2/feature/'+ pid+'.csv'\n",
    "        data_df=pd.read_csv(filename)\n",
    "        data_df=data_df.loc[:, data_df.columns != 'time']\n",
    "        data_df=data_df.fillna(0)\n",
    "        df=pd.DataFrame(np.vstack((df,data_df)),columns=df.columns)\n",
    "        print(df.columns)        \n",
    "        print(df.shape)\n",
    "        print(data_df.shape)\n",
    "        print(pid)\n",
    "    print(sum(df.isnull().sum(axis=1)))\n",
    "    df=df.dropna()\n",
    "    toc= timeit.default_timer()\n",
    "    print(toc-tic)\n",
    "    df.to_csv('data2/totaldata')\n",
    "    return df.iloc[:,:len(df.columns)-1],df.iloc[:,len(df.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd3ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when running for the first time\n",
    "pids=pd.read_csv('data/pids.txt',header=None)\n",
    "df, y=load_features_label(pids)\n",
    "#after it already ran once\n",
    "#dftot=pd.read_csv('data/totaldata')\n",
    "#df=dftot.iloc[:,1:len(dftot.columns)-1]\n",
    "#y=dftot.iloc[:,len(dftot.columns)-1]\n",
    "\n",
    "\n",
    "df=df[df.columns[np.sum(df,axis=0)!=0]]\n",
    "y=y.astype('int')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fa58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(df, y ,train_size=0.75, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896dfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of how to train the random forest for the full feature space\n",
    "tic= timeit.default_timer()\n",
    "clfRF = RandomForestClassifier(n_estimators=300,random_state=0, bootstrap=False)\n",
    "clfRF.fit(X_train, y_train)\n",
    "y_hat=clfRF.predict(X_test)\n",
    "df_clf_report=classification_report_df(y_test, y_hat, labels=[0, 1],name='300 trees with full feature space')  \n",
    "df_clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ac9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating PCA\n",
    "tic= timeit.default_timer()\n",
    "pca = PCA()\n",
    "data_pca=pca.fit_transform(df)\n",
    "principalComponents_xtrain, principalComponents_xtest, y_train, y_test  = train_test_split( data_pca, y ,train_size=0.75, random_state=0)\n",
    "print(pca.explained_variance_ratio_.round(3)[:20])\n",
    "toc = timeit.default_timer()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6905cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictELM(X_train,X_test,y_train,hidden_size, alpha=0,method='relu', bootstrap=1):\n",
    "    onehotencoder = OneHotEncoder(categories='auto')\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    y_train = onehotencoder.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    y_hat_tot=0\n",
    "    input_size = X_train.shape[1]\n",
    "    def relu(x,alpha):\n",
    "        return np.maximum(x, 0, x)+alpha*np.maximum(-x, 0, x)\n",
    "    def sine(x):\n",
    "        return np.sin(x)\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def hidden_nodes(X,method):\n",
    "        G = np.dot(X, input_weights)\n",
    "        G = G + biases\n",
    "        if method=='relu':\n",
    "            H = relu(G, alpha)\n",
    "        elif method=='sine':\n",
    "            H = sine(G)\n",
    "        elif method =='sigmoid':\n",
    "            H =sigmoid(G)\n",
    "        else:\n",
    "            print('no valid method')\n",
    "            return G\n",
    "        return H\n",
    "    def predict(X, method):\n",
    "        out = hidden_nodes(X,method)\n",
    "        out = np.dot(out, output_weights)\n",
    "        return out\n",
    "    for i in range(bootstrap):\n",
    "        input_weights = np.random.normal(size=[input_size,hidden_size])\n",
    "        biases = np.random.normal(size=[hidden_size])\n",
    "        output_weights = np.dot(scipy.linalg.pinv2(hidden_nodes(X_train, method)), y_train)\n",
    "        predicted = predict(X_test,method)\n",
    "        y_hat_i=np.argmax(predicted, axis=1)\n",
    "        y_hat_tot+=y_hat_i\n",
    "    y_hat_p=y_hat_tot/bootstrap\n",
    "    y_hat=np.logical_not(y_hat_p<=0.5)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309ffc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of ELM\n",
    "tic= timeit.default_timer()\n",
    "hidden_size=20\n",
    "y_hat=predictELM(X_train,X_test,y_train,hidden_size,0,'relu')\n",
    "df_clf_report=classification_report_df(y_test, y_hat, labels=[0, 1],name='elm '+str(hidden_size)+ ' hidden neurons and ReLu as activation')  \n",
    "df_clf_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b766c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using sequential floating feature selection \n",
    "tic= timeit.default_timer()\n",
    "model=RandomForestClassifier(n_estimators=10,random_state=0)\n",
    "sfs1 = SFS(model, \n",
    "           k_features=75, \n",
    "           forward=True, \n",
    "           floating=True, \n",
    "           scoring='accuracy',\n",
    "           cv=0,\n",
    "           verbose=5,\n",
    "          n_jobs=-1)\n",
    "\n",
    "sfs1 = sfs1.fit(X_train, y_train)\n",
    "print('\\nSequential Forward Floating Selection (k=75):')\n",
    "print(sfs1.k_feature_idx_)\n",
    "features_sffs=sfs1.k_feature_idx_\n",
    "print('CV Score:')\n",
    "print(sfs1.k_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b565ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_list=[]\n",
    "for i in X_train.columns.tolist():\n",
    "    cor = np.corrcoef(X_train[i].astype(float), y_train)[0, 1]\n",
    "    cor_list.append(cor)\n",
    "cor_feature = X_train.iloc[:,np.argsort(np.abs(cor_list))].columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search\n",
    "trees=[10,20,30,50,100,300]\n",
    "number_of_hidden_nodes=[5,10,15,20,25,50,100,200,500]\n",
    "number_of_features=[10,20,30,40,50,60,70,75,80,90,100]\n",
    "dfresultsRF=pd.DataFrame(trees)\n",
    "dfresultsRFPCA=dfresultsRF.set_index(0)\n",
    "dfresultsRFCORR=dfresultsRF.set_index(0)\n",
    "\n",
    "#random forest\n",
    "for j in number_of_features:\n",
    "    lst=[]\n",
    "    lst2=[]\n",
    "    for i in trees:\n",
    "        clf = RandomForestClassifier(n_estimators=i,random_state=0)\n",
    "        clf.fit(X_train[cor_feature[-j:]], y_train)\n",
    "        y_hat=clf.predict(X_test[cor_feature[-j:]])\n",
    "        df_clf_report=classification_report_df(y_test, y_hat, labels=[0, 1],name='RF '+str(i)+ ' trees and '+str(j)+' features selected on correlation ') \n",
    "        lst.append(df_clf_report['F1_score'][2])\n",
    "        clf.fit(principalComponents_xtrain[:,:j], y_train)\n",
    "        y_hat=clf.predict(principalComponents_xtest[:,:j])\n",
    "        df_clf_report2=classification_report_df(y_test, y_hat, labels=[0, 1],name='RF '+str(i)+ ' trees and '+str(j)+' features selected on pca' ) \n",
    "        lst2.append(df_clf_report2['F1_score'][2])\n",
    "    dfresultsRFPCA[str(j)+' feat corr RF']=lst\n",
    "    dfresultsRFCORR[str(j)+' feat trees PCA RF']=lst2\n",
    "#\n",
    "dfresults=pd.DataFrame(number_of_hidden_nodes)\n",
    "dfresultsRELUPCA=dfresults.set_index(0).copy()\n",
    "dfresultsSINEPCA=dfresults.set_index(0).copy()\n",
    "dfresultsSIGMOIDPCA=dfresults.set_index(0).copy()\n",
    "dfresultsRELUCORR=dfresults.set_index(0).copy()\n",
    "dfresultsSINECORR=dfresults.set_index(0).copy()\n",
    "dfresultsSIGMOIDCORR=dfresults.set_index(0).copy()\n",
    "dfresults=dfresults.set_index(0).copy()\n",
    "dfresultsRFsffs=pd.DataFrame(trees)\n",
    "k=0\n",
    "#ReLu\n",
    "for j in number_of_features:\n",
    "    lst=[]\n",
    "    lst2=[]\n",
    "    for i in number_of_hidden_nodes:\n",
    "        y_hat_cor=predictELM(X_train[cor_feature[-j:]],X_test[cor_feature[-j:]],y_train,i,k,'relu')\n",
    "        df_clf_report=classification_report_df(y_test, y_hat_cor, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on correlation relu leaky '+str(k)) \n",
    "        lst.append(df_clf_report['F1_score'][2])\n",
    "        y_hat_pca=predictELM(principalComponents_xtrain[:,:j],principalComponents_xtest[:,:j],y_train,i,k,'relu')\n",
    "        df_clf_report2=classification_report_df(y_test, y_hat_pca, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on pca relu leaky'+str(k)) \n",
    "        lst2.append(df_clf_report2['F1_score'][2])\n",
    "    dfresultsRELUPCA[str(j)+' feat '+str(k)+ ' leaky relu PCA ELM']=lst2\n",
    "    dfresultsRELUCORR[str(j)+' feat '+str(k)+ ' leaky relu corr ELM']=lst  \n",
    "#sine      \n",
    "k='sine'\n",
    "for j in number_of_features:\n",
    "    lst=[]\n",
    "    lst2=[]\n",
    "    for i in number_of_hidden_nodes:\n",
    "        y_hat_cor=predictELM(X_train[cor_feature[-j:]],X_test[cor_feature[-j:]],y_train,i,0,k)\n",
    "        df_clf_report=classification_report_df(y_test, y_hat_cor, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on correlation '+k) \n",
    "        lst.append(df_clf_report['F1_score'][2])\n",
    "        y_hat_pca=predictELM(principalComponents_xtrain[:,:j],principalComponents_xtest[:,:j],y_train,i,0,k)\n",
    "        df_clf_report2=classification_report_df(y_test, y_hat_pca, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on pca ' +k) \n",
    "        lst2.append(df_clf_report2['F1_score'][2])\n",
    "    dfresultsSINECORR[str(j)+' feat '+str(k)+' corr ELM']=lst\n",
    "    dfresultsSINEPCA[str(j)+' feat '+str(k)+' PCA ELM']=lst2\n",
    "#sigmoid\n",
    "k='sigmoid'\n",
    "for j in number_of_features:\n",
    "    lst=[]\n",
    "    lst2=[]\n",
    "    for i in number_of_hidden_nodes:\n",
    "        y_hat_cor=predictELM(X_train[cor_feature[-j:]],X_test[cor_feature[-j:]],y_train,i,0,k)\n",
    "        df_clf_report=classification_report_df(y_test, y_hat_cor, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on correlation '+k) \n",
    "        lst.append(df_clf_report['F1_score'][2])\n",
    "        y_hat_pca=predictELM(principalComponents_xtrain[:,:j],principalComponents_xtest[:,:j],y_train,i,0,k)\n",
    "        df_clf_report2=classification_report_df(y_test, y_hat_pca, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on pca ' +k) \n",
    "        lst2.append(df_clf_report2['F1_score'][2])\n",
    "    dfresultsSIGMOIDCORR[str(j)+' feat '+str(k)+' corr ELM']=lst\n",
    "    dfresultsSIGMOIDPCA[str(j)+' feat '+str(k)+' PCA ELM']=lst2\n",
    "                  \n",
    "dfresultsRELUsffs=dfresults.copy()\n",
    "dfresultsSINEsffs=dfresults.copy()\n",
    "dfresultsSIGMOIDsffs=dfresults.copy()\n",
    "j=75\n",
    "\n",
    "lst=[]\n",
    "for i in number_of_hidden_nodes:\n",
    "    y_hat_cor=predictELM(X_train[X_train.columns[list(features_sffs)]],X_test[X_test.columns[list(features_sffs)]],y_train,i,k,'relu')\n",
    "    df_clf_report=classification_report_df(y_test, y_hat_cor, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on sffs relu leaky '+str(k)) \n",
    "    lst.append(df_clf_report['F1_score'][2])\n",
    "dfresultsRELUsffs[str(j)+' feat '+str(k)+ ' leaky relu sffs ELM']=lst  \n",
    "       \n",
    "k='sine'\n",
    "\n",
    "lst=[]\n",
    "for i in number_of_hidden_nodes:\n",
    "    y_hat_cor=predictELM(X_train[X_train.columns[list(features_sffs)]],X_test[X_test.columns[list(features_sffs)]],y_train,i,0,k)\n",
    "    df_clf_report=classification_report_df(y_test, y_hat_cor, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on sffs '+k) \n",
    "    lst.append(df_clf_report['F1_score'][2])\n",
    "dfresultsSINEsffs[str(j)+' feat '+str(k)+' sffs ELM']=lst\n",
    "k='sigmoid'\n",
    "\n",
    "lst=[]\n",
    "for i in number_of_hidden_nodes:\n",
    "    y_hat_cor=predictELM(X_train[X_train.columns[list(features_sffs)]],X_test[X_test.columns[list(features_sffs)]],y_train,i,0,k)\n",
    "    df_clf_report=classification_report_df(y_test, y_hat_cor, labels=[0, 1],name='elm '+str(i)+ ' hidden neurons and '+str(j)+' features selected on sffs '+k) \n",
    "    lst.append(df_clf_report['F1_score'][2])\n",
    "dfresultsSIGMOIDsffs[str(j)+' feat '+str(k)+' sffs ELM']=lst\n",
    "\n",
    "lst=[]\n",
    "for i in trees:\n",
    "    clf = RandomForestClassifier(n_estimators=i,random_state=0)\n",
    "    clf.fit(X_train[X_train.columns[list(features_sffs)]], y_train)\n",
    "    y_hat=clf.predict(X_test[X_test.columns[list(features_sffs)]])\n",
    "    df_clf_report=classification_report_df(y_test, y_hat, labels=[0, 1],name='RF '+str(i)+ ' trees and '+str(j)+' features selected on sffs ') \n",
    "    lst.append(df_clf_report['F1_score'][2])\n",
    "dfresultsRFsffs[str(j)+' feat '+str(k)+' sffs RF']=lst   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bd1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFgraphPCA=dfresults.copy()\n",
    "DFgraphPCA['RELU 10 features']=dfresultsRELUPCABESTVAL\n",
    "DFgraphPCA['Sine 75 features']=dfresultsSINEPCA[dfresultsSINEPCA.max().idxmax()]\n",
    "DFgraphPCA['Sigmoid 75 features']=dfresultsSIGMOIDPCA[dfresultsSIGMOIDPCA.max().idxmax()]\n",
    "DFgraphPCA.plot(ylabel='F1 score',xlabel='number of hidden nodes',logx=True,legend=True, title='ELM with PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4dcbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFgraphCORR=dfresults.copy()\n",
    "DFgraphCORR['RELU 10 features']=dfresultsRELUCORRBESTVAL\n",
    "DFgraphCORR['Sine 80 features']=dfresultsSINECORR[dfresultsSINECORR.max().idxmax()]\n",
    "DFgraphCORR['Sigmoid 90 features']=dfresultsSIGMOIDCORR[dfresultsSIGMOIDCORR.max().idxmax()]\n",
    "DFgraphCORR.plot(ylabel='F1 score',xlabel='number of hidden nodes',logx=True,legend=True, title='ELM with correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84504ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFgraphsffs=dfresults.copy()\n",
    "DFgraphsffs=DFgraphsffs.set_index(0)\n",
    "DFgraphsffs['RELU']=dfresultsRELUsffs.set_index(0)\n",
    "DFgraphsffs['Sine']=dfresultsSINEsffs\n",
    "DFgraphsffs['Sigmoid']=dfresultsSIGMOIDsffs\n",
    "DFgraphsffs.plot(ylabel='F1 score',xlabel='number of hidden nodes',logx=True,legend=True, title='ELM with sffs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a0a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DFgraphRF=pd.DataFrame(trees).set_index(0)\n",
    "DFgraphRF['sffs']=dfresultsRFsffs['75 feat sigmoid sffs RF']\n",
    "DFgraphRF['PCA']=dfresultsRFPCA[dfresultsRFPCA.max().idxmax()]\n",
    "DFgraphRF['CORR']=dfresultsRFCORR[dfresultsRFCORR.max().idxmax()]\n",
    "DFgraphRF.plot(ylabel='F1 score',xlabel='Trees',logx=True,legend=True, title='Random forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81e4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
